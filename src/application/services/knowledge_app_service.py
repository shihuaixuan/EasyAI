"""
çŸ¥è¯†åº“åº”ç”¨æœåŠ¡
"""

import os
import logging
import re
from src.domain.knowledge.entities.document_chunk import DocumentChunk
from src.domain.knowledge.entities.knowledge_base import KnowledgeBase
from typing import List, Optional, Dict, Any
from datetime import datetime
from fastapi import UploadFile
from ..dto.knowledge_base_dto import (
    KnowledgeBaseCreateRequest,
    KnowledgeBaseResponse,
    KnowledgeBaseListResponse,
    KnowledgeBaseOverviewResponse
)
from ..dto.file_upload_dto import (
    FileUploadResponse,
    FileUploadBatchResponse,
    FileListBatchResponse,
    FileListResponse
)
from ..dto.workflow_dto import (
    WorkflowConfigRequest,
    WorkflowConfigResponse
)
from ...domain.knowledge.entities.knowledge_base import KnowledgeBase
from ...domain.knowledge.entities.document import Document
from ...domain.knowledge.entities.document_chunk import DocumentChunk
from ...domain.knowledge.repositories.knowledge_base_repository import KnowledgeBaseRepository
from ...domain.knowledge.repositories.document_repository import DocumentRepository
from ...domain.knowledge.services.knowledge_base_domain_service import KnowledgeBaseDomainService
from ...domain.knowledge.services.file_upload_service import FileUploadService, UploadedFile
from ...domain.knowledge.services.document_parser_service import DocumentParserService
from ...domain.knowledge.services.chunking.document_chunking_service import DocumentChunkingService
from ...domain.knowledge.repositories.document_chunk_repository import DocumentChunkRepository
from ...domain.knowledge.vo.workflow_config import FileUploadConfig
from ...domain.knowledge.vo.chunking_config import ChunkingConfig, TextPreprocessingConfig
from ...infrastructure.repositories.knowledge.embedding_vector_repository import EmbeddingVectorRepositoryImpl
from ...infrastructure.database import get_async_session
from ...domain.model.services.embedding.factory import create_embedding
from .embedding_application_service import create_embedding_application_service

logging.basicConfig(level=logging.DEBUG)


class KnowledgeApplicationService:
    """çŸ¥è¯†åº“åº”ç”¨æœåŠ¡"""
    
    def __init__(
        self,
        knowledge_base_domain_service: KnowledgeBaseDomainService,
        file_upload_service: FileUploadService,
        document_parser_service: DocumentParserService,
        document_chunking_service: DocumentChunkingService,
        knowledge_base_repo: KnowledgeBaseRepository,
        document_repo: DocumentRepository,
        document_chunk_repo: DocumentChunkRepository
    ):
        self.knowledge_base_domain_service = knowledge_base_domain_service
        self.file_upload_service = file_upload_service
        self.document_parser_service = document_parser_service
        self.document_chunking_service = document_chunking_service
        self.knowledge_base_repo = knowledge_base_repo
        self.document_repo = document_repo
        self.document_chunk_repo = document_chunk_repo
    
    async def create_knowledge_base(
        self, 
        request: KnowledgeBaseCreateRequest, 
        owner_id: str
    ) -> KnowledgeBaseResponse:
        """åˆ›å»ºçŸ¥è¯†åº“"""
        knowledge_base = await self.knowledge_base_domain_service.create_knowledge_base(
            name=request.name,
            description=request.description,
            owner_id=owner_id,
            config=request.config
        )
        
        return self._to_knowledge_base_response(knowledge_base)
    
    async def get_knowledge_base(self, knowledge_base_id: str) -> Optional[KnowledgeBaseResponse]:
        """è·å–çŸ¥è¯†åº“è¯¦æƒ…"""
        knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
        if not knowledge_base:
            return None
        
        return self._to_knowledge_base_response(knowledge_base)
    
    async def list_knowledge_bases(self, owner_id: str) -> KnowledgeBaseListResponse:
        """è·å–ç”¨æˆ·çš„çŸ¥è¯†åº“åˆ—è¡¨"""
        knowledge_bases = await self.knowledge_base_repo.find_active_by_owner_id(owner_id)
        
        responses = [self._to_knowledge_base_response(kb) for kb in knowledge_bases]
        
        return KnowledgeBaseListResponse(
            knowledge_bases=responses,
            total=len(responses)
        )
    
    async def get_knowledge_base_overview(
        self, 
        knowledge_base_id: str
    ) -> KnowledgeBaseOverviewResponse:
        """è·å–çŸ¥è¯†åº“æ¦‚è§ˆ"""
        overview = await self.knowledge_base_domain_service.get_knowledge_base_overview(
            knowledge_base_id
        )
        
        return KnowledgeBaseOverviewResponse(
            knowledge_base=self._to_knowledge_base_response(overview["knowledge_base"]),
            document_count=overview["document_count"],
            chunk_count=overview["chunk_count"],
            recent_documents=[self._document_to_dict(doc) for doc in overview["recent_documents"]],
            file_types=overview["file_types"],
            total_size=overview["total_size"]
        )
    
    async def update_workflow_config(
        self, 
        knowledge_base_id: str, 
        config_request: WorkflowConfigRequest,
        user_id: str
    ) -> WorkflowConfigResponse:
        """æ›´æ–°å·¥ä½œæµé…ç½®"""
        try:
            # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨å¹¶æ£€æŸ¥æƒé™
            knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
            if not knowledge_base:
                raise ValueError(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}")
            if knowledge_base.owner_id != user_id:
                raise ValueError(f"æ— æƒé™è®¿é—®çŸ¥è¯†åº“: {knowledge_base_id}")
            
            # å°†è¯·æ±‚è½¬æ¢ä¸ºé…ç½®å­—å…¸
            config_dict = {
                "chunking": {
                    "strategy": config_request.chunking.strategy,
                    "separator": config_request.chunking.separator,
                    "max_length": config_request.chunking.max_length,
                    "overlap_length": config_request.chunking.overlap_length,
                    "preprocessing": {
                        "remove_extra_whitespace": config_request.chunking.remove_extra_whitespace,
                        "remove_urls": config_request.chunking.remove_urls,
                        "remove_emails": config_request.chunking.remove_emails  # æ–°å¢å­—æ®µ
                    }
                },
                "embedding": {
                    "strategy": config_request.embedding.strategy,
                    "model_name": config_request.embedding.model_name
                },
                "retrieval": {
                    "strategy": config_request.retrieval.strategy,
                    "top_k": config_request.retrieval.top_k,
                    "score_threshold": config_request.retrieval.score_threshold,
                    "enable_rerank": config_request.retrieval.enable_rerank,
                    "rerank_model": config_request.retrieval.rerank_model
                }
            }
            
            # æ·»åŠ çˆ¶å­åˆ†æ®µç‰¹æœ‰é…ç½®
            if config_request.chunking.strategy == "parent_child":
                config_dict["chunking"].update({
                    "parent_separator": config_request.chunking.parent_separator,
                    "parent_max_length": config_request.chunking.parent_max_length,
                    "child_separator": config_request.chunking.child_separator,
                    "child_max_length": config_request.chunking.child_max_length
                })
            
            # æ›´æ–°é…ç½®ï¼ˆä½¿ç”¨æ”¹è¿›åçš„æ–¹æ³•ï¼‰
            updated_knowledge_base = await self.knowledge_base_domain_service.update_knowledge_base_config(
                knowledge_base_id, config_dict
            )
            
            # éªŒè¯é…ç½®æ›´æ–°æ˜¯å¦æˆåŠŸ
            if not updated_knowledge_base.config:
                raise ValueError("é…ç½®æ›´æ–°å¤±è´¥ï¼šæ•°æ®åº“ä¸­æ²¡æœ‰ä¿å­˜é…ç½®")
            
            # è®°å½•é…ç½®æ›´æ–°æˆåŠŸ
            print(f"é…ç½®æ›´æ–°æˆåŠŸï¼šçŸ¥è¯†åº“ {knowledge_base_id}, æ–°é…ç½®: {config_dict}")
            
            # å¼‚æ­¥è§¦å‘æ–‡æ¡£åˆ†å—å¤„ç†ï¼ˆä¸é˜»å¡é…ç½®æ›´æ–°ï¼‰
            try:
                # æ£€æŸ¥æ˜¯å¦æœ‰æœªå¤„ç†çš„æ–‡ä»¶ï¼ˆä»æ–‡ä»¶ç³»ç»Ÿæ‰«æï¼‰
                upload_dir = f"uploads/{knowledge_base_id}"
                if os.path.exists(upload_dir):
                    uploaded_files = [f for f in os.listdir(upload_dir) if os.path.isfile(os.path.join(upload_dir, f))]
                    if uploaded_files:
                        print(f"å¼€å§‹å¤„ç† {len(uploaded_files)} ä¸ªæœªå¤„ç†çš„æ–‡ä»¶")
                        # å¼‚æ­¥å¤„ç†æ‰€æœ‰æœªå¤„ç†çš„æ–‡ä»¶
                        asyncio.create_task(self._process_unprocessed_documents_async(
                            knowledge_base_id, config_dict, user_id
                        ))
                    else:
                        print(f"çŸ¥è¯†åº“ {knowledge_base_id} æ²¡æœ‰æœªå¤„ç†çš„æ–‡ä»¶")
                else:
                    print(f"çŸ¥è¯†åº“ {knowledge_base_id} çš„ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨")
            except Exception as process_error:
                # æ–‡æ¡£å¤„ç†å¤±è´¥ä¸å½±å“é…ç½®æ›´æ–°çš„æˆåŠŸ
                print(f"è­¦å‘Šï¼šæ–‡æ¡£å¤„ç†å¤±è´¥ - {str(process_error)}")
            
            # è¿”å›å“åº”
            return WorkflowConfigResponse(
                knowledge_base_id=knowledge_base_id,
                config=updated_knowledge_base.config,
                updated_at=updated_knowledge_base.updated_at or datetime.now()
            )
            
        except ValueError as ve:
            raise ve
        except Exception as e:
            raise ValueError(f"æ›´æ–°å·¥ä½œæµé…ç½®å¤±è´¥: {str(e)}")
    
    async def upload_file(
        self, 
        knowledge_base_id: str, 
        file: UploadFile
    ) -> FileUploadResponse:
        """ä¸Šä¼ å•ä¸ªæ–‡ä»¶"""
        try:
            # æ£€æŸ¥æ–‡ä»¶åæ˜¯å¦ä¸ºç©º
            if not file.filename:
                return FileUploadResponse(
                    success=False,
                    filename="unknown",
                    error_message="æ–‡ä»¶åä¸èƒ½ä¸ºç©º"
                )
            
            # è¯»å–æ–‡ä»¶å†…å®¹
            content: bytes = await file.read()
            
            # åˆ›å»ºä¸Šä¼ æ–‡ä»¶å¯¹è±¡
            uploaded_file = UploadedFile(
                filename=file.filename,
                content=content,
                content_type=file.content_type or "application/octet-stream",
                size=len(content)
            )
            
            # ä¿å­˜æ–‡ä»¶
            upload_result = await self.file_upload_service.save_file(
                uploaded_file, knowledge_base_id
            )
            
            if not upload_result.success or not upload_result.file_path:
                return FileUploadResponse(
                    success=False,
                    filename=file.filename or "unknown",
                    error_message=upload_result.error_message or "æ–‡ä»¶ä¸Šä¼ å¤±è´¥"
                )
            
            # è§£ææ–‡æ¡£ï¼ˆä½†ä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
            try:
                document = await self.document_parser_service.parse_document(
                    upload_result.file_path,
                    upload_result.filename or file.filename or "unknown",
                    knowledge_base_id
                )
                
                # è®¾ç½®æ–‡æ¡£å±æ€§ï¼ˆä½†ä¸ä¿å­˜åˆ°æ•°æ®åº“ï¼‰
                document.original_path = upload_result.file_path
                if upload_result.file_size is not None:
                    document.file_size = upload_result.file_size
                document.content_hash = upload_result.content_hash
                
                # æ³¨æ„ï¼šæ­¤æ—¶ä¸ä¿å­˜æ–‡æ¡£åˆ°æ•°æ®åº“ï¼Œåªä¿ç•™æ–‡ä»¶å’ŒåŸºæœ¬ä¿¡æ¯
                # æ–‡æ¡£å°†åœ¨ç”¨æˆ·ç‚¹å‡»"å¼€å§‹åˆ†å—"æ—¶ä¸åˆ†å—æ•°æ®ä¸€èµ·ä¿å­˜åˆ°æ•°æ®åº“
                
                return FileUploadResponse(
                    success=True,
                    filename=file.filename or "unknown",
                    file_path=upload_result.file_path,
                    document_id=None,  # æ­¤æ—¶è¿˜æ²¡æœ‰æ•°æ®åº“ID
                    file_size=upload_result.file_size,
                    content_hash=upload_result.content_hash,
                    created_at=None,  # æ­¤æ—¶è¿˜æ²¡æœ‰åˆ›å»ºæ—¶é—´
                    chunk_count=0  # åˆå§‹ä¸Šä¼ æ—¶æ²¡æœ‰åˆ†å—
                )
                
            except Exception as parse_error:
                # è§£æå¤±è´¥ï¼Œåˆ é™¤å·²ä¸Šä¼ çš„æ–‡ä»¶
                if upload_result.file_path:
                    await self.file_upload_service.delete_file(upload_result.file_path)
                
                return FileUploadResponse(
                    success=False,
                    filename=file.filename or "unknown",
                    error_message=f"æ–‡æ¡£è§£æå¤±è´¥: {str(parse_error)}"
                )
            
        except Exception as e:
            return FileUploadResponse(
                success=False,
                filename=file.filename or "unknown",
                error_message=f"æ–‡ä»¶ä¸Šä¼ å¤±è´¥: {str(e)}"
            )
    
    async def upload_files_batch(
        self, 
        knowledge_base_id: str, 
        files: List[UploadFile]
    ) -> FileUploadBatchResponse:
        """æ‰¹é‡ä¸Šä¼ æ–‡ä»¶"""
        successful_uploads = []
        failed_uploads = []
        
        # é€ä¸ªå¤„ç†æ–‡ä»¶ï¼Œæ¯ä¸ªæ–‡ä»¶åœ¨ç‹¬ç«‹äº‹åŠ¡ä¸­å¤„ç†
        for file in files:
            result = await self.upload_file(knowledge_base_id, file)
            
            if result.success:
                successful_uploads.append(result)
            else:
                failed_uploads.append(result)
        
        return FileUploadBatchResponse(
            total_files=len(files),
            successful_uploads=successful_uploads,
            failed_uploads=failed_uploads,
            success_count=len(successful_uploads),
            error_count=len(failed_uploads)
        )
    
    async def list_files(self, knowledge_base_id: str, user_id: str) -> FileListBatchResponse:
        """è·å–çŸ¥è¯†åº“æ–‡ä»¶åˆ—è¡¨"""
        # éªŒè¯ç”¨æˆ·æƒé™
        knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
        if not knowledge_base:
            raise ValueError("çŸ¥è¯†åº“ä¸å­˜åœ¨")
        if knowledge_base.owner_id != user_id:
            raise ValueError("æ— æƒé™è®¿é—®æ­¤çŸ¥è¯†åº“")
        
        documents = await self.document_repo.find_by_knowledge_base_id(knowledge_base_id)
        
        file_responses = []
        for doc in documents:
            file_responses.append(FileListResponse(
                document_id=doc.document_id or "",
                filename=doc.filename,
                file_type=doc.file_type,
                file_size=doc.file_size,
                is_processed=doc.is_processed,
                chunk_count=doc.chunk_count,
                created_at=doc.created_at or datetime.now(),
                processed_at=doc.processed_at
            ))
        
        return FileListBatchResponse(
            knowledge_base_id=knowledge_base_id,
            files=file_responses,
            total=len(file_responses)
        )
    
    async def delete_knowledge_base(self, knowledge_base_id: str, user_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“"""
        # éªŒè¯ç”¨æˆ·æƒé™
        knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
        if not knowledge_base:
            return False
        if knowledge_base.owner_id != user_id:
            raise ValueError("æ— æƒé™åˆ é™¤æ­¤çŸ¥è¯†åº“")
        
        return await self.knowledge_base_domain_service.delete_knowledge_base(knowledge_base_id)
    
    def _to_knowledge_base_response(self, knowledge_base: KnowledgeBase) -> KnowledgeBaseResponse:
        """è½¬æ¢ä¸ºçŸ¥è¯†åº“å“åº”å¯¹è±¡"""
        return KnowledgeBaseResponse(
            knowledge_base_id=knowledge_base.knowledge_base_id or "",
            name=knowledge_base.name,
            description=knowledge_base.description,
            owner_id=knowledge_base.owner_id or "",
            config=knowledge_base.config or {},
            is_active=knowledge_base.is_active,
            document_count=knowledge_base.document_count,
            chunk_count=knowledge_base.chunk_count,
            created_at=knowledge_base.created_at or datetime.now(),
            updated_at=knowledge_base.updated_at or datetime.now()
        )
    
    def _document_to_dict(self, document: Document) -> Dict[str, Any]:
        """è½¬æ¢æ–‡æ¡£ä¸ºå­—å…¸"""
        return {
            "document_id": document.document_id,
            "filename": document.filename,
            "file_type": document.file_type,
            "file_size": document.file_size,
            "is_processed": document.is_processed,
            "chunk_count": document.chunk_count,
            "created_at": document.created_at.isoformat() if document.created_at else None
        }
    
    async def start_knowledge_processing(
        self, 
        knowledge_base_id: str,
        user_id: str = None
    ) -> Dict[str, Any]:
        """å¼€å§‹çŸ¥è¯†åº“å¤„ç†æµç¨‹ï¼ˆå¤„ç†å·²ä¸Šä¼ ä½†æœªå…¥åº“çš„æ–‡ä»¶ï¼‰"""
        try:
            # 1. è·å–çŸ¥è¯†åº“å’Œé…ç½®ï¼ˆåœ¨äº‹åŠ¡å¼€å§‹å‰éªŒè¯ï¼‰
            knowledge_base: KnowledgeBase | None = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
            if not knowledge_base:
                raise ValueError(f"çŸ¥è¯†åº“ä¸å­˜åœ¨: {knowledge_base_id}")
            
            if not knowledge_base.config:
                raise ValueError("çŸ¥è¯†åº“é…ç½®ä¸ºç©ºï¼Œè¯·å…ˆé…ç½®å·¥ä½œæµå‚æ•°")
            
            # 2. æ‰«æuploadsç›®å½•ä¸­çš„æ–‡ä»¶ï¼Œä½†åªå¤„ç†æœªå…¥åº“çš„æ–‡ä»¶
            # TODO: æœªæ¥ä¼šæœ‰ä¸“é—¨å­˜å‚¨æ–‡ä»¶çš„å®ç°ï¼Œæ¯”å¦‚ï¼šminio/oss
            upload_dir = f"uploads/{knowledge_base_id}"
            if not os.path.exists(upload_dir):
                logging.info(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {upload_dir}")
                return {
                    "success": True,
                    "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶",
                    "processed_documents": 0,
                    "total_chunks": 0
                }
            
            # è·å–å·²å…¥åº“çš„æ–‡æ¡£åˆ—è¡¨
            existing_documents = await self.document_repo.find_by_knowledge_base_id(knowledge_base_id)
            existing_docs_by_filename = {doc.filename: doc for doc in existing_documents}
            
            logging.info(f"ğŸ“‹ çŸ¥è¯†åº“ä¸­å·²æœ‰ {len(existing_documents)} ä¸ªæ–‡æ¡£")
            
            # æ‰«æuploadsç›®å½•ï¼Œæ™ºèƒ½å¤„ç†æ–‡ä»¶
            uploaded_files = []
            files_to_delete = []  # éœ€è¦åˆ é™¤çš„æ—§æ–‡æ¡£
            
            for filename in os.listdir(upload_dir):
                file_path = os.path.join(upload_dir, filename)
                if not os.path.isfile(file_path):
                    continue
                
                # è®¡ç®—æ–°æ–‡ä»¶çš„hash
                with open(file_path, 'rb') as f:
                    content = f.read()
                    new_file_hash = hashlib.sha256(content).hexdigest()
                
                # æ£€æŸ¥æ˜¯å¦æœ‰åŒåæ–‡ä»¶
                if filename in existing_docs_by_filename:
                    existing_doc = existing_docs_by_filename[filename]
                    
                    if existing_doc.content_hash == new_file_hash:
                        # hashç›¸åŒï¼ŒçœŸæ­£é‡å¤ï¼Œè·³è¿‡
                        logging.info(f"â­ï¸  æ–‡ä»¶å†…å®¹ç›¸åŒï¼Œè·³è¿‡: {filename} (hash: {new_file_hash[:16]}...)")
                        continue
                    else:
                        # hashä¸åŒï¼Œæ–‡ä»¶å·²æ›´æ–°ï¼Œéœ€è¦åˆ é™¤æ—§æ•°æ®
                        logging.info(f"ğŸ”„ æ–‡ä»¶å·²æ›´æ–°ï¼Œå°†åˆ é™¤æ—§æ•°æ®: {filename}")
                        files_to_delete.append(existing_doc)
                
                # æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨
                uploaded_files.append({
                    'filename': filename,
                    'file_path': file_path,
                    'file_size': os.path.getsize(file_path),
                    'content_hash': new_file_hash
                })
                logging.info(f"ğŸ“„ å‘ç°å¾…å¤„ç†æ–‡ä»¶: {filename}")
            
            # åˆ é™¤éœ€è¦æ›´æ–°çš„æ—§æ–‡æ¡£åŠå…¶ç›¸å…³æ•°æ®
            if files_to_delete:
                logging.info(f"ğŸ—‘ï¸  åˆ é™¤ {len(files_to_delete)} ä¸ªæ—§æ–‡æ¡£åŠå…¶æ•°æ®...")
                for old_doc in files_to_delete:
                    await self._delete_document_and_related_data(old_doc.document_id)
            
            logging.info(f"ğŸ” æ‰¾åˆ° {len(uploaded_files)} ä¸ªå¾…å¤„ç†æ–‡ä»¶")
            
            if not uploaded_files:
                return {
                    "success": True,
                    "message": "æ²¡æœ‰æ‰¾åˆ°éœ€è¦å¤„ç†çš„æ–‡ä»¶",
                    "processed_documents": 0,
                    "total_chunks": 0
                }
            
            # 3. ä½¿ç”¨é€šç”¨å¤„ç†é€»è¾‘
            return await self._process_file_list(knowledge_base, uploaded_files, user_id)
            
        except ValueError as ve:
            # éªŒè¯é”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
            raise ve
        except Exception as e:
            # å…¶ä»–å¼‚å¸¸ï¼ŒåŒ…è£…åæŠ›å‡º
            raise Exception(f"å¤„ç†æµç¨‹å¤±è´¥: {str(e)}")
    
    async def _process_file_list(
        self,
        knowledge_base: KnowledgeBase,
        uploaded_files: List[Dict[str, Any]],
        user_id: str = None
    ) -> Dict[str, Any]:
        """
        å¤„ç†æ–‡ä»¶åˆ—è¡¨çš„é€šç”¨é€»è¾‘
        """
        knowledge_base_id = knowledge_base.knowledge_base_id
        total_chunks = 0
        processed_documents = 0
        failed_documents = []
        
        chunking_config = knowledge_base.config.get('chunking', {})
        
        for file_info in uploaded_files:
            try:
                # è§£ææ–‡æ¡£
                document = await self.document_parser_service.parse_document(
                    file_info['file_path'],
                    file_info['filename'],
                    knowledge_base_id
                )
                
                # æ¸…ç†æ–‡æ¡£å†…å®¹ä¸­çš„æ— æ•ˆå­—ç¬¦
                document.content = self._clean_text_content(document.content)
                
                # è®¾ç½®æ–‡æ¡£å±æ€§
                document.original_path = file_info['file_path']
                document.file_size = file_info['file_size']
                
                # ä½¿ç”¨é¢„è®¡ç®—çš„hashå€¼ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
                if 'content_hash' in file_info:
                    document.content_hash = file_info['content_hash']
                else:
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œï¼ˆç”¨äºå»é‡æ£€æŸ¥ï¼‰
                    with open(file_info['file_path'], 'rb') as f:
                        content = f.read()
                        document.content_hash = hashlib.sha256(content).hexdigest()
                
                # åœ¨åŒä¸€äº‹åŠ¡ä¸­ä¿å­˜æ–‡æ¡£å’Œåˆ†å—
                saved_document, chunks_count = await self._save_document_and_chunks_in_transaction(
                    knowledge_base_id, document, chunking_config, file_info['file_path']
                )
                
                total_chunks += chunks_count
                if chunks_count > 0:
                    processed_documents += 1
                    print(f"âœ… æ–‡ä»¶ {file_info['filename']} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {chunks_count} ä¸ªåˆ†å—")
                else:
                    print(f"â­ï¸  æ–‡ä»¶ {file_info['filename']} å·²å­˜åœ¨ï¼Œè·³è¿‡å¤„ç†")
                
                # è®°å½•éœ€è¦å¤„ç†embeddingçš„æ–‡æ¡£ï¼ˆå»¶è¿Ÿåˆ°äº‹åŠ¡æäº¤åï¼‰
                # åªå¯¹æ–°å¤„ç†çš„æ–‡æ¡£å¯åŠ¨embeddingä»»åŠ¡
                if saved_document.document_id and chunks_count > 0:
                    # ä½¿ç”¨ä¼ å…¥çš„ç”¨æˆ·IDï¼Œå¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨çŸ¥è¯†åº“çš„æ‰€æœ‰è€…ID
                    effective_user_id = user_id or knowledge_base.owner_id
                    if not effective_user_id:
                        raise ValueError("æ— æ³•ç¡®å®šç”¨æˆ·IDï¼Œè¯·ç¡®ä¿å·²ç™»å½•")
                    
                    # æ·»åŠ åˆ°å¾…å¤„ç†åˆ—è¡¨ï¼Œè€Œä¸æ˜¯ç«‹å³å¯åŠ¨å¼‚æ­¥ä»»åŠ¡
                    if not hasattr(self, '_pending_embedding_tasks'):
                        self._pending_embedding_tasks = []
                    self._pending_embedding_tasks.append({
                        'knowledge_base_id': knowledge_base_id,
                        'document_id': saved_document.document_id,
                        'user_id': effective_user_id
                    })
                    print(f"ğŸ“ æ–‡æ¡£ {file_info['filename']} å·²åŠ å…¥embeddingå¤„ç†é˜Ÿåˆ—")
                
            except Exception as e:
                print(f"å¤„ç†æ–‡ä»¶ {file_info['filename']} å¤±è´¥: {str(e)}")
                failed_documents.append({
                    "filename": file_info['filename'],
                    "error": str(e)
                })
                # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œä¸ä¸­æ–­æ•´ä¸ªæµç¨‹
                continue
        
        # 4. æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯ï¼ˆåœ¨å•ç‹¬çš„æ“ä½œä¸­ï¼Œé¿å…äº‹åŠ¡å†²çªï¼‰
        try:
            await self.knowledge_base_domain_service.update_knowledge_base_statistics(knowledge_base_id)
        except Exception as stats_error:
            print(f"è­¦å‘Šï¼šæ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥ - {str(stats_error)}")
            # ç»Ÿè®¡æ›´æ–°å¤±è´¥ä¸å½±å“ä¸»æµç¨‹
        
        # 5. å¾…å¤„ç†çš„embeddingä»»åŠ¡å°†åœ¨controllerä¸­äº‹åŠ¡æäº¤åå¯åŠ¨
        
        return {
            "success": True,
            "message": f"å¤„ç†å®Œæˆï¼Œå…±å¤„ç† {processed_documents} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {total_chunks} ä¸ªæ–‡æœ¬å—",
            "processed_documents": processed_documents,
            "total_chunks": total_chunks,
            "failed_documents": failed_documents
        }
    
    async def _delete_document_and_related_data(self, document_id: str) -> None:
        """
        åˆ é™¤æ–‡æ¡£åŠå…¶ç›¸å…³çš„æ‰€æœ‰æ•°æ®ï¼ˆchunkså’Œembeddingsï¼‰
        
        Args:
            document_id: æ–‡æ¡£ID
        """
        try:
            print(f"ğŸ—‘ï¸  å¼€å§‹åˆ é™¤æ–‡æ¡£åŠç›¸å…³æ•°æ®: {document_id}")
            
            # 1. åˆ é™¤embeddingsï¼ˆé€šè¿‡document_idæ‰¹é‡åˆ é™¤ï¼‰
            try:
                async with get_async_session() as session:
                    embedding_repo = EmbeddingVectorRepositoryImpl(session)
                    deleted_embeddings = await embedding_repo.delete_embeddings_by_document(document_id)
                    print(f"   åˆ é™¤äº† {deleted_embeddings} ä¸ªembeddings")
                    await session.commit()
            except Exception as e:
                print(f"   åˆ é™¤embeddingså¤±è´¥: {str(e)}")
            
            # 2. åˆ é™¤chunks
            deleted_chunks = await self.document_chunk_repo.delete_by_document_id(document_id)
            print(f"   åˆ é™¤äº† {deleted_chunks} ä¸ªchunks")
            
            # 3. åˆ é™¤æ–‡æ¡£
            success = await self.document_repo.delete_by_id(document_id)
            if success:
                print(f"   âœ… æ–‡æ¡£åˆ é™¤æˆåŠŸ: {document_id}")
            else:
                print(f"   âš ï¸  æ–‡æ¡£åˆ é™¤å¤±è´¥: {document_id}")
                
        except Exception as e:
            print(f"âŒ åˆ é™¤æ–‡æ¡£æ•°æ®å¤±è´¥: {document_id}, é”™è¯¯: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸ç»§ç»­å¤„ç†å…¶ä»–æ–‡ä»¶
    
    def _clean_text_content(self, content: str) -> str:
        """
        æ¸…ç†æ–‡æœ¬å†…å®¹ä¸­çš„æ— æ•ˆå­—ç¬¦
        
        Args:
            content: åŸå§‹æ–‡æœ¬å†…å®¹
            
        Returns:
            æ¸…ç†åçš„æ–‡æœ¬å†…å®¹
        """
        if not content:
            return content
        
        try:
            # 1. ç§»é™¤ç©ºå­—èŠ‚å’Œå…¶ä»–æ§åˆ¶å­—ç¬¦
            content = content.replace('\x00', '')  # ç§»é™¤ç©ºå­—èŠ‚
            content = content.replace('\x0b', '')  # ç§»é™¤å‚ç›´åˆ¶è¡¨ç¬¦
            content = content.replace('\x0c', '')  # ç§»é™¤æ¢é¡µç¬¦
            
            # 2. ç§»é™¤å…¶ä»–ä¸å¯æ‰“å°çš„æ§åˆ¶å­—ç¬¦ï¼ˆä¿ç•™å¸¸ç”¨çš„æ¢è¡Œç¬¦ã€åˆ¶è¡¨ç¬¦ç­‰ï¼‰
            # ä¿ç•™å¸¸ç”¨çš„ç©ºç™½å­—ç¬¦ï¼šç©ºæ ¼ã€åˆ¶è¡¨ç¬¦ã€æ¢è¡Œç¬¦ã€å›è½¦ç¬¦
            content = re.sub(r'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x9f]', '', content)
            
            # 3. ç¡®ä¿å†…å®¹æ˜¯æœ‰æ•ˆçš„UTF-8
            content = content.encode('utf-8', errors='ignore').decode('utf-8')
            
            # 4. æ¸…ç†å¤šä½™çš„ç©ºç™½å­—ç¬¦
            content = re.sub(r'\n{3,}', '\n\n', content)  # æœ€å¤šä¿ç•™ä¸¤ä¸ªè¿ç»­æ¢è¡Œ
            content = re.sub(r'[ \t]+', ' ', content)     # å¤šä¸ªç©ºæ ¼/åˆ¶è¡¨ç¬¦åˆå¹¶ä¸ºä¸€ä¸ªç©ºæ ¼
            
            return content.strip()
            
        except Exception as e:
            print(f"âš ï¸  æ–‡æœ¬æ¸…ç†å¤±è´¥ï¼Œä½¿ç”¨åŸå§‹å†…å®¹: {str(e)}")
            # å¦‚æœæ¸…ç†å¤±è´¥ï¼Œè‡³å°‘ç§»é™¤ç©ºå­—èŠ‚
            return content.replace('\x00', '') if content else content
    
    async def _process_single_document(
        self, 
        document: Document, 
        config: Dict[str, Any],
        user_id: str = None
    ) -> int:
        """å¤„ç†å•ä¸ªæ–‡æ¡£ï¼ˆåˆ†å— + embedding + å­˜å‚¨ï¼‰"""
        try:
            # 1. è·å–æ–‡æ¡£å†…å®¹
            content = await self._get_document_content(document)
            if not content:
                raise ValueError(f"æ— æ³•è·å–æ–‡æ¡£å†…å®¹: {document.filename}")
            
            # 2. æ–‡æœ¬åˆ†å—
            chunks = await self._chunk_document_content(
                content, config.get('chunking', {})
            )
            
            if not chunks:
                raise ValueError(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {document.filename}")
            
            # 3. æ¸…ç†æ—§çš„åˆ†å—æ•°æ®
            await self.document_chunk_repo.delete_by_document_id(document.document_id or "")
            
            # 4. ç”Ÿæˆå‘é‡å¹¶å­˜å‚¨åˆ†å—
            embedding_config = config.get('embedding', {})
            saved_chunks = await self._process_and_store_chunks(
                chunks, document, embedding_config, user_id=user_id
            )
            
            return len(saved_chunks)
            
        except Exception as e:
            raise Exception(f"å¤„ç†æ–‡æ¡£ {document.filename} å¤±è´¥: {str(e)}")
    
    async def _get_document_content(self, document: Document) -> str:
        """è·å–æ–‡æ¡£å†…å®¹"""
        try:
            # ä»æ–‡ä»¶è·¯å¾„è¯»å–å†…å®¹
            if document.file_path and os.path.exists(document.file_path):
                with open(document.file_path, 'r', encoding='utf-8') as f:
                    return f.read()
            else:
                # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œå°è¯•ä»æ•°æ®åº“ä¸­è·å–
                return document.content or ""
        except Exception as e:
            raise Exception(f"è¯»å–æ–‡æ¡£å†…å®¹å¤±è´¥: {str(e)}")
    
    async def _chunk_document_content(
        self, 
        content: str, 
        chunking_config: Dict[str, Any]
    ) -> List[Dict[str, Any]]:
        """å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†å—"""
        try:
            # åˆ›å»ºåˆ†å—é…ç½®å¯¹è±¡
            config = ChunkingConfig.from_dict(chunking_config)
            
            # ä½¿ç”¨åˆ†å—æœåŠ¡è¿›è¡Œåˆ†å—
            chunks_result = await self.document_chunking_service.chunk_text(content, config)
            
            # è½¬æ¢ä¸ºç»Ÿä¸€æ ¼å¼
            chunks = []
            for i, chunk_text in enumerate(chunks_result.chunks):
                chunks.append({
                    "content": chunk_text,
                    "chunk_index": i,
                    "start_index": 0,  # ç®€åŒ–å¤„ç†
                    "end_index": len(chunk_text),
                    "metadata": {
                        "chunk_method": config.strategy,
                        "chunk_size": len(chunk_text)
                    }
                })
            
            return chunks
            
        except Exception as e:
            raise Exception(f"æ–‡æ¡£åˆ†å—å¤±è´¥: {str(e)}")
    
    async def _process_and_store_chunks(
        self, 
        chunks: List[Dict[str, Any]], 
        document: Document,
        embedding_config: Dict[str, Any],
        knowledge_base: Any = None,
        user_id: str = None
    ) -> List[DocumentChunk]:
        """å¤„ç†å¹¶å­˜å‚¨æ–‡æœ¬å—ï¼ˆåŒ…æ‹¬ç”Ÿæˆå‘é‡ï¼‰"""
        
        saved_chunks = []
        
        # åˆå§‹åŒ–embeddingæœåŠ¡ï¼ˆå¦‚æœéœ€è¦ï¼‰
        embedding_service = None
        if embedding_config.get('strategy') == 'high_quality':
            # ä½¿ç”¨ä¼ å…¥çš„é…ç½®åˆ›å»ºembeddingæœåŠ¡
            embedding_service = await self._create_embedding_service(embedding_config)
        
        for chunk_data in chunks:
            try:
                # åˆ›å»ºæ–‡æœ¬å—å®ä½“
                chunk = DocumentChunk(
                    content=chunk_data["content"],
                    chunk_index=chunk_data["chunk_index"],
                    start_offset=chunk_data["start_index"],
                    document_id=document.document_id or "",
                    knowledge_base_id=document.knowledge_base_id,
                    end_offset=chunk_data["end_index"],
                    metadata=chunk_data["metadata"]
                )
                
                # ç”Ÿæˆå‘é‡ï¼ˆå¦‚æœéœ€è¦ï¼‰
                if embedding_service:
                    try:
                        embedding = await embedding_service.embed_text(chunk_data["content"])
                        chunk.set_vector(embedding)
                    except Exception as e:
                        print(f"è­¦å‘Š: ç”Ÿæˆå‘é‡å¤±è´¥ {str(e)}, ç»§ç»­ä¿å­˜æ–‡æœ¬å—")
                
                # ä¿å­˜æ–‡æœ¬å—
                saved_chunk = await self.document_chunk_repo.save(chunk)
                saved_chunks.append(saved_chunk)
                
            except Exception as e:
                print(f"è­¦å‘Š: å¤„ç†æ–‡æœ¬å—å¤±è´¥ {str(e)}, è·³è¿‡è¯¥å—")
                continue
        
        # æ¸…ç†embeddingæœåŠ¡
        if embedding_service:
            try:
                # æ£€æŸ¥embeddingæœåŠ¡æ˜¯å¦æœ‰closeæ–¹æ³•
                if hasattr(embedding_service, 'close') and callable(getattr(embedding_service, 'close')):
                    close_method = getattr(embedding_service, 'close')
                    if asyncio.iscoroutinefunction(close_method):
                        await close_method()
                    else:
                        close_method()
            except Exception as close_error:
                print(f"è­¦å‘Šï¼šå…³é—­embeddingæœåŠ¡å¤±è´¥ - {str(close_error)}")
        
        return saved_chunks
    
    async def _create_embedding_service(self, embedding_config: Dict[str, Any]):
        """åˆ›å»ºembeddingæœåŠ¡"""
        try:
            
            # è·å–é…ç½®ä¿¡æ¯
            model_name = embedding_config.get('model_name', 'BAAI/bge-large-zh-v1.5')
            provider = embedding_config.get('provider', 'siliconflow')
            api_key = embedding_config.get('api_key')
            base_url = embedding_config.get('base_url')
            
            # æ„å»ºé…ç½®
            config = {
                'model_name': model_name,
                'batch_size': embedding_config.get('batch_size', 32),
                'max_tokens': embedding_config.get('max_tokens', 8192),
                'timeout': embedding_config.get('timeout', 30),
            }
            
            # æ ¹æ®æä¾›å•†æ·»åŠ ç‰¹å®šé…ç½®
            if provider == 'local':
                config.update({
                    'model_path': embedding_config.get('model_path', '/opt/embedding/bge-m3'),
                    'normalize_embeddings': embedding_config.get('normalize_embeddings', True)
                })
            else:
                # è¿œç¨‹APIæä¾›å•†
                config.update({
                    'api_key': api_key or os.getenv(f'{provider.upper()}_API_KEY'),
                    'base_url': base_url or self._get_default_base_url(provider)
                })
            
            print(f"åˆ›å»ºembeddingæœåŠ¡: provider={provider}, model={model_name}")
            embedding_service = create_embedding(provider, config)
            return embedding_service
            
        except Exception as e:
            raise Exception(f"åˆ›å»ºembeddingæœåŠ¡å¤±è´¥: {str(e)}")
    
    def _get_default_base_url(self, provider: str) -> str:
        """è·å–æä¾›å•†çš„é»˜è®¤base_url"""
        default_urls = {
            'siliconflow': 'https://api.siliconflow.cn/v1',
            'openai': 'https://api.openai.com/v1',
            'anthropic': 'https://api.anthropic.com',
        }
        return default_urls.get(provider, 'https://api.siliconflow.cn/v1')
    
    async def _process_document_embeddings_async(
        self, 
        knowledge_base_id: str, 
        document_id: str,
        user_id: str
    ) -> None:
        """
        å¼‚æ­¥å¤„ç†æ–‡æ¡£çš„embeddingï¼ˆå…è®¸å¤±è´¥ï¼‰
        
        Args:
            knowledge_base_id: çŸ¥è¯†åº“ID
            document_id: æ–‡æ¡£ID
            user_id: ç”¨æˆ·ID
        """
        try:
            print(f"ğŸš€ å¼€å§‹å¼‚æ­¥å¤„ç†æ–‡æ¡£ {document_id} çš„embedding...")
            
            # ä½¿ç”¨æ–°çš„DDDæ¶æ„çš„embeddingåº”ç”¨æœåŠ¡
            
            async with get_async_session() as session:
                embedding_app_service = await create_embedding_application_service(session)
                result = await embedding_app_service.process_document_embeddings(
                    knowledge_base_id, document_id, user_id
                )
                
                if result.success:
                    print(f"âœ… æ–‡æ¡£ {document_id} embeddingå¤„ç†æˆåŠŸ: {result.message}")
                    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {result.processed_chunks} ä¸ªåˆ†å—ï¼Œå¤±è´¥ {result.failed_chunks} ä¸ªåˆ†å—")
                else:
                    print(f"âŒ æ–‡æ¡£ {document_id} embeddingå¤„ç†å¤±è´¥: {result.message}")
                    print(f"ğŸ“Š å¤„ç†ç»Ÿè®¡: æˆåŠŸ {result.processed_chunks} ä¸ªåˆ†å—ï¼Œå¤±è´¥ {result.failed_chunks} ä¸ªåˆ†å—")
                    print(f"ğŸ’¡ å»ºè®®æ£€æŸ¥embeddingæ¨¡å‹é…ç½®å’ŒAPIå¯†é’¥")
                
        except Exception as e:
            print(f"å¼‚æ­¥embeddingå¤„ç†å‡ºé”™: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œå…è®¸å¤±è´¥
    
    async def process_documents_chunking(
        self, 
        knowledge_base_id: str, 
        chunking_config: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        å¯¹çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£è¿›è¡Œåˆ†å—å¤„ç†
        
        Args:
            knowledge_base_id: çŸ¥è¯†åº“ID
            chunking_config: åˆ†å—é…ç½®
            
        Returns:
            å¤„ç†ç»“æœç»Ÿè®¡
        """
        try:
            # è·å–çŸ¥è¯†åº“ä¸­çš„æ‰€æœ‰æ–‡æ¡£
            documents = await self.document_repo.find_by_knowledge_base_id(knowledge_base_id)
            
            if not documents:
                return {"processed_documents": 0, "total_chunks": 0, "message": "æ²¡æœ‰æ‰¾åˆ°æ–‡æ¡£"}
            
            # åˆ›å»ºåˆ†å—é…ç½®å¯¹è±¡
            config = ChunkingConfig.from_dict(chunking_config)
            
            total_chunks = 0
            processed_documents = 0
            
            for document in documents:
                try:
                    # æ¸…ç†æ—§çš„åˆ†å—
                    await self.document_chunk_repo.delete_by_document_id(document.document_id or "")
                    
                    # è¿›è¡Œæ–°çš„åˆ†å—å¤„ç†
                    chunks = await self.document_chunking_service.chunk_document(document, config)
                    
                    # ä¿å­˜åˆ†å—
                    if chunks:
                        await self.document_chunk_repo.save_batch(chunks)
                        total_chunks += len(chunks)
                    
                    # æ›´æ–°æ–‡æ¡£çš„åˆ†å—ç»Ÿè®¡
                    document.chunk_count = len(chunks)
                    document.mark_as_processed(len(chunks))
                    await self.document_repo.update(document)
                    
                    processed_documents += 1
                    
                except Exception as e:
                    print(f"å¤„ç†æ–‡æ¡£ {document.filename} æ—¶å‡ºé”™: {str(e)}")
                    continue
            
            return {
                "processed_documents": processed_documents,
                "total_chunks": total_chunks,
                "message": f"æˆåŠŸå¤„ç† {processed_documents} ä¸ªæ–‡æ¡£ï¼Œç”Ÿæˆ {total_chunks} ä¸ªåˆ†å—"
            }
            
        except Exception as e:
            return {
                "processed_documents": 0,
                "total_chunks": 0,
                "error": f"åˆ†å—å¤„ç†å¤±è´¥: {str(e)}"
            }
    
    async def _save_document_and_chunks_in_transaction(
        self, 
        knowledge_base_id: str, 
        document: Document,
        chunking_config: Dict[str, Any],
        file_path: Optional[str] = None
    ) -> tuple[Document, int]:
        """
        åœ¨åŒä¸€äº‹åŠ¡ä¸­ä¿å­˜æ–‡æ¡£å’Œåˆ†å—
        
        Args:
            knowledge_base_id: çŸ¥è¯†åº“ID
            document: æ–‡æ¡£å¯¹è±¡
            chunking_config: åˆ†å—é…ç½®
            file_path: æ–‡ä»¶è·¯å¾„ï¼ˆç”¨äºç›´æ¥è¯»å–æ–‡ä»¶å†…å®¹ï¼‰
            
        Returns:
            tuple[Document, int]: ä¿å­˜çš„æ–‡æ¡£å’Œåˆ†å—æ•°é‡
        """
        try:
            # 1. ä½¿ç”¨é¢†åŸŸæœåŠ¡ä¿å­˜æ–‡æ¡£ï¼ˆåŒ…å«å»é‡æ£€æŸ¥ï¼‰
            try:
                saved_document = await self.knowledge_base_domain_service.add_document_to_knowledge_base(
                    knowledge_base_id, document
                )
                logging.info(f"âœ… æ–‡æ¡£ä¿å­˜æˆåŠŸ: {document.filename}")
            except ValueError as e:
                if "æ–‡æ¡£å†…å®¹é‡å¤" in str(e):
                    logging.info(f"âš ï¸  æ–‡æ¡£å†…å®¹é‡å¤ï¼Œè·³è¿‡: {document.filename}")
                    # æŸ¥æ‰¾å·²å­˜åœ¨çš„æ–‡æ¡£
                    existing_doc = await self.document_repo.find_by_content_hash(
                        document.content_hash, knowledge_base_id
                    )
                    if existing_doc:
                        return existing_doc, 0  # è¿”å›å·²å­˜åœ¨çš„æ–‡æ¡£ï¼Œåˆ†å—æ•°ä¸º0
                raise e
            
            # 2. è·å–æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ†å—
            content = ""
            if file_path and os.path.exists(file_path):
                # ä»æ–‡ä»¶è¯»å–å†…å®¹
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                except UnicodeDecodeError:
                    # å¦‚æœ UTF-8 è§£ç å¤±è´¥ï¼Œå°è¯•å…¶ä»–ç¼–ç 
                    try:
                        with open(file_path, 'r', encoding='gbk') as f:
                            content = f.read()
                    except UnicodeDecodeError:
                        with open(file_path, 'r', encoding='latin-1') as f:
                            content = f.read()
            else:
                # å¦‚æœæ²¡æœ‰æ–‡ä»¶è·¯å¾„ï¼Œä½¿ç”¨æ–‡æ¡£å¯¹è±¡ä¸­çš„å†…å®¹
                content = saved_document.content or ""
            
            if not content.strip():
                print(f"è­¦å‘Šï¼šæ–‡æ¡£ {document.filename} å†…å®¹ä¸ºç©ºï¼Œè·³è¿‡åˆ†å—å¤„ç†")
                return saved_document, 0
            
            # 3. è¿›è¡Œæ–‡æ¡£åˆ†å—
            chunks = await self._chunk_document_content(content, chunking_config)
            
            # 4. ä¿å­˜åˆ†å—ï¼ˆä¸åŒ…å«å‘é‡ï¼‰
            document_chunks = []
            for chunk_data in chunks:
                chunk = DocumentChunk(
                    content=chunk_data["content"],
                    chunk_index=chunk_data["chunk_index"],
                    start_offset=chunk_data["start_index"],
                    document_id=saved_document.document_id or "",
                    knowledge_base_id=knowledge_base_id,
                    end_offset=chunk_data["end_index"],
                    metadata=chunk_data["metadata"]
                )
                document_chunks.append(chunk)
            
            # 5. æ‰¹é‡ä¿å­˜åˆ†å—
            if document_chunks:
                saved_chunks: List[DocumentChunk] = await self.document_chunk_repo.save_batch(document_chunks)
                chunks_count: int = len(saved_chunks)
            else:
                chunks_count = 0
            
            # 6. æ›´æ–°æ–‡æ¡£çš„åˆ†å—ç»Ÿè®¡
            saved_document.mark_as_processed(chunk_count=chunks_count)
            await self.document_repo.update(saved_document)
            
            # 7. æ³¨æ„ï¼šäº‹åŠ¡æäº¤ç”±è°ƒç”¨æ–¹å¤„ç†
            
            return saved_document, chunks_count
            
        except Exception as e:
            raise Exception(f"ä¿å­˜æ–‡æ¡£å’Œåˆ†å—å¤±è´¥: {str(e)}")
    
    async def _process_unprocessed_documents_async(
        self, 
        knowledge_base_id: str, 
        config: Dict[str, Any],
        user_id: str
    ) -> None:
        """
        å¼‚æ­¥å¤„ç†æœªå¤„ç†çš„æ–‡æ¡£ï¼ˆä»æ–‡ä»¶ç³»ç»Ÿè¯»å–æ–‡ä»¶ï¼Œæ–‡æ¡£è½åº“+åˆ†å—åœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼‰
        
        Args:
            knowledge_base_id: çŸ¥è¯†åº“ID
            config: å®Œæ•´çš„å·¥ä½œæµé…ç½®
            user_id: ç”¨æˆ·IDï¼Œç”¨äºéªŒè¯æƒé™
        """
        try:
            print(f"å¼€å§‹å¼‚æ­¥å¤„ç†çŸ¥è¯†åº“ {knowledge_base_id} çš„æ–‡ä»¶...")
            
            # 1. éªŒè¯ç”¨æˆ·å¯¹çŸ¥è¯†åº“çš„æƒé™
            knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
            if not knowledge_base:
                print(f"çŸ¥è¯†åº“ {knowledge_base_id} ä¸å­˜åœ¨")
                return
            if knowledge_base.owner_id != user_id:
                print(f"ç”¨æˆ· {user_id} æ— æƒé™è®¿é—®çŸ¥è¯†åº“ {knowledge_base_id}")
                return
            
            # 2. ä»æ–‡ä»¶ç³»ç»Ÿæ‰«ææœªå¤„ç†çš„æ–‡ä»¶
            upload_dir = f"uploads/{knowledge_base_id}"
            if not os.path.exists(upload_dir):
                print(f"ä¸Šä¼ ç›®å½•ä¸å­˜åœ¨: {upload_dir}")
                return
            
            # è·å–å·²ä¸Šä¼ çš„æ–‡ä»¶åˆ—è¡¨
            uploaded_files = []
            for filename in os.listdir(upload_dir):
                file_path = os.path.join(upload_dir, filename)
                if os.path.isfile(file_path):
                    uploaded_files.append({
                        'filename': filename,
                        'file_path': file_path,
                        'file_size': os.path.getsize(file_path)
                    })
            
            if not uploaded_files:
                print(f"çŸ¥è¯†åº“ {knowledge_base_id} æ²¡æœ‰æœªå¤„ç†çš„æ–‡ä»¶")
                return
            
            print(f"å‘ç° {len(uploaded_files)} ä¸ªæœªå¤„ç†çš„æ–‡ä»¶")
            
            # 2. æå–åˆ†å—å’ŒåµŒå…¥é…ç½®
            chunking_config = config.get('chunking', {})
            embedding_config = config.get('embedding', {})
            
            # 3. å¤„ç†æ¯ä¸ªæ–‡ä»¶ï¼ˆæ–‡æ¡£è½åº“+åˆ†å—åœ¨åŒä¸€äº‹åŠ¡ä¸­ï¼‰
            total_chunks = 0
            processed_count = 0
            failed_count = 0
            
            for file_info in uploaded_files:
                try:
                    print(f"å¤„ç†æ–‡ä»¶: {file_info['filename']}")
                    
                    # è§£ææ–‡æ¡£
                    document = await self.document_parser_service.parse_document(
                        file_info['file_path'],
                        file_info['filename'],
                        knowledge_base_id
                    )
                    
                    # è®¾ç½®æ–‡æ¡£å±æ€§
                    document.original_path = file_info['file_path']
                    document.file_size = file_info['file_size']
                    
                    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
                    with open(file_info['file_path'], 'rb') as f:
                        content = f.read()
                        document.content_hash = hashlib.sha256(content).hexdigest()
                    
                    # åœ¨åŒä¸€äº‹åŠ¡ä¸­ä¿å­˜æ–‡æ¡£å’Œåˆ†å—
                    saved_document, chunks_count = await self._save_document_and_chunks_in_transaction(
                        knowledge_base_id, document, chunking_config, file_info['file_path']
                    )
                    
                    total_chunks += chunks_count
                    processed_count += 1
                    
                    print(f"æ–‡ä»¶ {file_info['filename']} å¤„ç†å®Œæˆï¼Œç”Ÿæˆ {chunks_count} ä¸ªåˆ†å—")
                    
                    # å¼‚æ­¥å¤„ç†åµŒå…¥ï¼ˆæ£€æŸ¥çŸ¥è¯†åº“çš„å®é™…embeddingé…ç½®ï¼‰
                    try:
                        # è·å–çŸ¥è¯†åº“çš„å®é™…é…ç½®
                        kb = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
                        if kb and kb.config:
                            kb_embedding_config = kb.config.get('embedding', {})
                            strategy = kb_embedding_config.get('strategy', 'economic')
                            print(f"ğŸ“‹ çŸ¥è¯†åº“embeddingç­–ç•¥: {strategy}")
                            
                            if strategy == 'high_quality':
                                print(f"ğŸš€ å¯åŠ¨å¼‚æ­¥embeddingå¤„ç†...")
                                asyncio.create_task(self._process_embeddings_async(
                                    saved_document.document_id or "", knowledge_base_id
                                ))
                            else:
                                print(f"âš ï¸  è·³è¿‡embeddingå¤„ç†ï¼Œç­–ç•¥ä¸º: {strategy}")
                        else:
                            print(f"âŒ çŸ¥è¯†åº“æ²¡æœ‰embeddingé…ç½®ï¼Œè·³è¿‡embeddingå¤„ç†")
                    except Exception as embedding_error:
                        print(f"âŒ æ£€æŸ¥embeddingé…ç½®å¤±è´¥: {str(embedding_error)}")
                    
                except Exception as file_error:
                    print(f"å¤„ç†æ–‡ä»¶ {file_info['filename']} å¤±è´¥: {str(file_error)}")
                    failed_count += 1
                    # ç»§ç»­å¤„ç†ä¸‹ä¸€ä¸ªæ–‡ä»¶ï¼Œè€Œä¸æ˜¯ä¸­æ–­æ•´ä¸ªæµç¨‹
                    continue
            
            # 4. æ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡
            try:
                await self.knowledge_base_domain_service.update_knowledge_base_statistics(knowledge_base_id)
            except Exception as stats_error:
                print(f"è­¦å‘Šï¼šæ›´æ–°çŸ¥è¯†åº“ç»Ÿè®¡å¤±è´¥ - {str(stats_error)}")
            
            print(f"æ–‡æ¡£å¤„ç†å®Œæˆï¼šæˆåŠŸ {processed_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ªï¼Œæ€»åˆ†å—æ•° {total_chunks}")
            
        except Exception as e:
            print(f"å¼‚æ­¥æ–‡æ¡£å¤„ç†å¤±è´¥: {str(e)}")
            # æ³¨æ„ï¼šå¼‚æ­¥ä»»åŠ¡ä¸­çš„å¼‚å¸¸ä¸ä¼šå½±å“ä¸»æµç¨‹ï¼Œä½†éœ€è¦ç¡®ä¿ä¸ä¼šå¯¼è‡´äº‹åŠ¡é—®é¢˜
    
    async def _process_embeddings_async(
        self, 
        document_id: str, 
        knowledge_base_id: str
    ) -> None:
        """
        å¼‚æ­¥å¤„ç†æ–‡æ¡£åˆ†å—çš„embeddingï¼ˆå…è®¸å¤±è´¥ï¼‰
        
        Args:
            document_id: æ–‡æ¡£ID
            knowledge_base_id: çŸ¥è¯†åº“ID
        """
        try:
            print(f"å¼€å§‹å¤„ç†æ–‡æ¡£ {document_id} çš„embedding...")
            
            # 1. è·å–çŸ¥è¯†åº“é…ç½®
            knowledge_base = await self.knowledge_base_repo.find_by_id(knowledge_base_id)
            if not knowledge_base or not knowledge_base.config:
                print(f"è­¦å‘Šï¼šçŸ¥è¯†åº“ {knowledge_base_id} æ²¡æœ‰é…ç½®ï¼Œè·³è¿‡embeddingå¤„ç†")
                return
            
            embedding_config = knowledge_base.config.get('embedding', {})
            if not embedding_config or embedding_config.get('strategy') != 'high_quality':
                print(f"è­¦å‘Šï¼šçŸ¥è¯†åº“ {knowledge_base_id} æ²¡æœ‰å¯ç”¨é«˜è´¨é‡embeddingï¼Œè·³è¿‡å¤„ç†")
                return
            
            # 2. è·å–æ–‡æ¡£åˆ†å—
            chunks = await self.document_chunk_repo.find_by_document_id(document_id)
            if not chunks:
                print(f"è­¦å‘Šï¼šæ–‡æ¡£ {document_id} æ²¡æœ‰åˆ†å—æ•°æ®")
                return
            
            # 3. åˆ›å»º embedding æœåŠ¡
            embedding_service = await self._create_embedding_service(embedding_config)
            if not embedding_service:
                print(f"è­¦å‘Šï¼šæ— æ³•åˆ›å»º embedding æœåŠ¡")
                return
            
            # 4. ä¸ºæ¯ä¸ªåˆ†å—ç”Ÿæˆå‘é‡
            success_count = 0
            failed_count = 0
            
            for chunk in chunks:
                try:
                    if chunk.vector:
                        continue  # å·²ç»æœ‰å‘é‡äº†ï¼Œè·³è¿‡
                    
                    embedding = await embedding_service.embed_text(chunk.content)
                    chunk.set_vector(embedding)
                    await self.document_chunk_repo.update(chunk)
                    success_count += 1
                    
                except Exception as chunk_error:
                    print(f"è­¦å‘Šï¼šå¤„ç†åˆ†å— {chunk.chunk_id} çš„embeddingå¤±è´¥: {str(chunk_error)}")
                    failed_count += 1
                    continue
            
            print(f"Embeddingå¤„ç†å®Œæˆï¼šæˆåŠŸ {success_count} ä¸ªï¼Œå¤±è´¥ {failed_count} ä¸ª")
            
            # 5. æ¸…ç† embedding æœåŠ¡
            if embedding_service:
                try:
                    if hasattr(embedding_service, 'close') and callable(getattr(embedding_service, 'close')):
                        close_method = getattr(embedding_service, 'close')
                        if asyncio.iscoroutinefunction(close_method):
                            await close_method()
                        else:
                            close_method()
                except Exception as close_error:
                    print(f"è­¦å‘Šï¼šå…³é—­embeddingæœåŠ¡å¤±è´¥ - {str(close_error)}")
            
        except Exception as e:
            print(f"è­¦å‘Šï¼šEmbeddingå¼‚æ­¥å¤„ç†å¤±è´¥: {str(e)}")
            # ä¸æŠ›å‡ºå¼‚å¸¸ï¼Œä¸å½±å“ä¸»æµç¨‹